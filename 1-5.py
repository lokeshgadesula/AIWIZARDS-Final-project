import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

# Load data
data = np.array([
    (8.517720474290075, 16.39836792240312),
    (0.4491851329933081, 3.3390000109051114),
    (4.264870413619764, 9.714836650372424),
    (9.113012328478936, 16.02904868768032),
    (6.425348615884043, 11.367408147999233),
    (2.224728990628717, 5.170536914532143),
    (7.145892124218072, 15.4681068646142),
    (7.592529010231668, 15.77700445992232),
    (4.248892218244206, 10.497346117793757),
    (1.427087720438906, 2.3532431263503124),
    (2.2646028466818064, 6.0577403893707),
    (7.828236549538759, 13.59895823787207),
    (2.0470381786367375, 5.36347669281394),
    (7.090653128111822, 13.978359357015222),
    (8.238574606079107, 16.477074206281983),
    (9.806123021989238, 19.19236387935345),
    (4.839455993884413, 11.018994364253452),
    (0.9334432635744219, 1.359985340703215),
    (8.237590174968808, 17.585866190330513),
    (5.282882304765331, 11.47667378632162),
    (0.6036396402822262, 2.125047055132204),
    (3.0321867265147135, 6.741405389407541),
    (5.921334416298669, 12.717854890059431),
    (1.9901805064157108, 6.377530930522013),
    (2.487024004762094, 5.225050524852705),
    (0.7351297991447603, 2.7275079373491913),
    (6.111554256048809, 12.526125179218724),
    (6.279425085318087, 11.896093990945019),
    (2.684106535077263, 7.1027900899744285),
    (9.60967516020287, 18.58057371286072),
    (6.206882417600279, 12.282579169517172),
    (0.8053506705739075, 4.127153658431555),
    (8.020357460922234, 13.971265685891456),
    (5.895477703852401, 13.057218103702144),
    (1.718799344245626, 2.291138847337322),
    (1.9456666703312726, 4.289556447596815),
    (8.20887198727383, 16.502910055607305),
    (9.083824069056659, 17.873124260652585),
    (8.798849186859533, 18.499379866432843),
    (4.94373628636873, 11.445859962017284),
    (7.837314425184082, 15.292534312964575),
    (0.9399272295064703, 1.0590537153612178),
    (4.060587289691205, 7.241946511652467),
    (5.8132203258979604, 10.683043719262646),
    (9.058706443989081, 18.588363142139293),
    (3.3321043422839014, 5.334717371754672),
    (4.743190312125954, 10.260478005701641),
    (5.0287595513566, 11.779550611499403),
    (5.7010886645787915, 12.654783692011385),
    (7.95279676722688, 15.70813690021933),
    (5.254606476590608, 11.774166287511335),
    (8.265381658302358, 15.027246585816762),
    (6.412528372799226, 12.402265959645742),
    (7.949679691808021, 15.750265328733033),
    (9.49881329742581, 18.112808484191483),
    (6.173935551092682, 13.551253340276416),
    (5.162262257481582, 8.573144635352064),
    (3.1313897611091157, 8.277508295797338),
    (7.0123046029530895, 13.877986232902506),
    (8.397826726535131, 17.05424946760689),
    (1.808485825617689, 5.96013465796373),
    (3.208460684102789, 8.28296123540405),
    (6.551727773786582, 13.643768069461272),
    (6.971944478732437, 13.808090126784432),
    (9.079093220908511, 19.15851205412915),
    (0.15257160153446533, 1.6201241476706802),
    (8.194809812811192, 17.139568702658607),
    (8.225662865199375, 17.090688074205893),
    (0.8046490491380845, 1.2135994710647133),
    (9.061368363927231, 18.85579624153131),
    (6.056805469390859, 9.90877146415699),
    (1.850375610450366, 4.393087112925498),
    (4.234633170050943, 9.109181254941927),
    (5.978170042680259, 13.726821226372007),
    (0.5087606803201914, 2.1713103035593604),
    (8.22164433910902, 17.5052564103262),
    (6.356665436537954, 10.698238365830438),
    (1.8558716957216366, 4.905121780523498),
    (9.140416116846513, 18.490813618417548),
    (1.200653891128593, 5.019071844368574),
    (5.626213557689051, 12.240943270839887),
    (7.954127731448854, 16.226931960028317),
    (5.777251777874954, 10.733101022453521),
    (2.7660293792976263, 5.74277979861381),
    (9.983944118769807, 19.803969860051566),
    (2.5154647399780723, 4.397084151978797),
    (7.069307505050214, 15.538984317775661),
    (5.349514986894943, 11.85246849800015),
    (0.12540076197386035, 0.5504486114788241),
    (6.494376066793332, 11.082931129711486),
    (0.3512745563385028, 0.6646860325379245),
    (3.9494998576849145, 7.077132956163577),
    (0.13095345494167468, 0.6992180812323363),
    (1.7879321471550553, 5.451280411595844),
    (8.983874959282575, 18.82344276125042),
    (7.7965917583863575, 14.956149105219854),
    (2.121540666477777, 6.377407854574997),
    (4.026778893709581, 8.987765858716703),
    (9.130339777528079, 16.554635472149485),
    (3.449319475901613, 6.066318454174355)
])

X_data = data[:, 0]
Y_data = data[:, 1]

# Model parameters
a = tf.Variable(0.0, name='a')
b = tf.Variable(0.0, name='b')
c = tf.Variable(0.0, name='c')

# Learning rate and optimizer
learning_rate = 0.01
optimizer = tf.optimizers.SGD(learning_rate)

# Training function
def train_step(x, y):
    with tf.GradientTape() as tape:
        predictions = a * x**2 + b * x + c  # Quadratic model
        loss = tf.reduce_mean((y - predictions)**2)  # Mean squared error
    gradients = tape.gradient(loss, [a, b, c])
    optimizer.apply_gradients(zip(gradients, [a, b, c]))
    return loss

# Training loop with visualization
epochs = 100
for epoch in range(epochs):
    loss = train_step(X_data, Y_data)

    # Plot updates for every epoch
    plt.scatter(X_data, Y_data, color='blue', label='Data Points')
    X_model = np.linspace(min(X_data), max(X_data), 100)
    Y_model = a.numpy() * X_model**2 + b.numpy() * X_model + c.numpy()
    plt.plot(X_model, Y_model, color='red', label='Fitting Curve')
    plt.title(f'Epoch {epoch+1}, Loss: {loss.numpy():.4f}')
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.legend()
    plt.pause(0.1)
    plt.clf()  # Clear the plot for the next epoch

plt.close()
